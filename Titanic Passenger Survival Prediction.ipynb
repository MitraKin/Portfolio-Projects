{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:24:02.122619Z","iopub.execute_input":"2024-10-19T14:24:02.123177Z","iopub.status.idle":"2024-10-19T14:24:03.196961Z","shell.execute_reply.started":"2024-10-19T14:24:02.123108Z","shell.execute_reply":"2024-10-19T14:24:03.195414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Loading the datasets\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:34:53.197216Z","iopub.execute_input":"2024-10-19T14:34:53.197999Z","iopub.status.idle":"2024-10-19T14:34:54.552357Z","shell.execute_reply.started":"2024-10-19T14:34:53.197946Z","shell.execute_reply":"2024-10-19T14:34:54.550956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combining both train and test datasets for preprocessing\ntest['Survived'] = np.nan  # Adding Survived column in the test set for uniformity\ncombined = pd.concat([train, test], sort=False)\n\n# Data Preprocessing\n\n# Filling missing 'Embarked' values with the mode\ncombined['Embarked'].fillna(combined['Embarked'].mode()[0], inplace=True)\n\n# Filling missing 'Fare' values with the median\ncombined['Fare'].fillna(combined['Fare'].median(), inplace=True)\n\n# Extracting titles from names (e.g., Mr, Mrs, Miss, etc.)\ncombined['Title'] = combined['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n\n# Simplifying titles\ncombined['Title'] = combined['Title'].replace(['Mlle', 'Ms'], 'Miss')\ncombined['Title'] = combined['Title'].replace(['Mme', 'Lady', 'Countess', 'Dona'], 'Mrs')\ncombined['Title'] = combined['Title'].replace(['Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer'], 'Mr')\n\n# Filling missing 'Age' values based on the median age of corresponding Title groups\ncombined['Age'] = combined.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:37:53.772060Z","iopub.execute_input":"2024-10-19T14:37:53.773182Z","iopub.status.idle":"2024-10-19T14:37:53.802645Z","shell.execute_reply.started":"2024-10-19T14:37:53.773129Z","shell.execute_reply":"2024-10-19T14:37:53.801405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dropping irrelevant columns\ncombined.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis=1, inplace=True)\n\n# Encoding categorical variables\nlabel_encoder = LabelEncoder()\ncombined['Sex'] = label_encoder.fit_transform(combined['Sex'])\ncombined['Embarked'] = label_encoder.fit_transform(combined['Embarked'])\ncombined['Title'] = label_encoder.fit_transform(combined['Title'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:38:35.872328Z","iopub.execute_input":"2024-10-19T14:38:35.872758Z","iopub.status.idle":"2024-10-19T14:38:35.886488Z","shell.execute_reply.started":"2024-10-19T14:38:35.872715Z","shell.execute_reply":"2024-10-19T14:38:35.885110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separate the datasets back into train and test\ntrain_cleaned = combined[combined['Survived'].notna()]\ntest_cleaned = combined[combined['Survived'].isna()].drop('Survived', axis=1)\n\n# Define features and target variable\nX_train = train_cleaned.drop('Survived', axis=1)\ny_train = train_cleaned['Survived']\nX_test = test_cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:39:00.717775Z","iopub.execute_input":"2024-10-19T14:39:00.718510Z","iopub.status.idle":"2024-10-19T14:39:00.730172Z","shell.execute_reply.started":"2024-10-19T14:39:00.718461Z","shell.execute_reply":"2024-10-19T14:39:00.728840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random Forest with GridSearchCV for hyperparameter tuning\n\n# Random Forest Classifier\nrf = RandomForestClassifier(random_state=42)\n\n# Hyperparameter grid for tuning\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\ngrid_search.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:39:34.519430Z","iopub.execute_input":"2024-10-19T14:39:34.519874Z","iopub.status.idle":"2024-10-19T14:41:12.690289Z","shell.execute_reply.started":"2024-10-19T14:39:34.519831Z","shell.execute_reply":"2024-10-19T14:41:12.688890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the best estimator\nbest_rf = grid_search.best_estimator_\n\n# Predict on the test set\ntest_predictions = best_rf.predict(X_test)\n\n# Prepare the submission file\nsubmission = pd.DataFrame({\n    'PassengerId': test['PassengerId'],\n    'Survived': test_predictions.astype(int)\n})\n\n# Save the submission file as CSV\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:42:01.658297Z","iopub.execute_input":"2024-10-19T14:42:01.658783Z","iopub.status.idle":"2024-10-19T14:42:01.694482Z","shell.execute_reply.started":"2024-10-19T14:42:01.658735Z","shell.execute_reply":"2024-10-19T14:42:01.693061Z"}},"outputs":[],"execution_count":null}]}